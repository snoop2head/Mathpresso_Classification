# 1127_회의록



### QTID 전체 기준 training

중단원이 아니라 문제에서 속한 유형으로 연결시키면 더 좋을 것 같다. 어떤 걸 예측하냐에 따라서 모델 파일을 따로따로 올리자.

* 중단원 기준(qtid 일부)이 아니라 소단원 기준(qtid 전체 혹은 name) 592개 class를 갖고 multi-label classification하자.
* 소단원 기준으로 stratified random sampling을 진행하는 게 좋을 것 같다.

### Augmentation

* 현재: 몽데이크 데이터셋을 23000개로 불림 -> Preprocessing -> training, valid dataset으로 8:2로 나눔.
* 미래: Training dataset만 augmentation을 해야 validation dataset에 같은 문제들이 섞이지 않을 것. 즉 모듈로 만들어야겠네.

### Stratified Random Sampling

* Class 별로 비율이 유지되게 뽑는 거라서 hidden_reference랑 유사한 eval dataset인 듯.
* 소단원 기준으로 stratified random sampling을 진행하는 게 좋을 것 같다.
* **Random Sampling Seed가 컴퓨터 별로 다르게 나오는 경우가 있다. Train File, Validation File을 각각 csv파일로 하나 정해놓고 표준화시키자.** 

### F1 Score vs Loss

- [x] Optimizer는 아니지만 제가 지환님이랑 early stopping 관해서 보고 있는데요, 지환님이 지금 early stopping 기준이 loss로 되어있는데 이 기준을 f1 score로 바꾸는게 좋을 것 같다고 하시는데 다들 어떻게 생각하시나요??
  - [x] 그냥 loss대로 할게요 ㅠ
  - [x] [대부분 Loss를 갖고 최적화하는 게 일반적임.](https://www.kaggle.com/c/liverpool-ion-switching/discussion/147343)

### Model Training

* **Electra 경량화 모델을 사용하면서, 이 정도 성능이 나온다는 걸 보여주는 게 좋지 않을까?**
* **RNN, LSTM은 사이즈가 작은 모델이니까 이걸로 비교해보는 것도 나쁘지 않을 것 같다.**
* **Word2vec 혹은 Tfidf으로 만들어진 벡터를 Random forest로 사용할 수 있지 않을까?**



## Feedback

* Task에 매몰되지 않았으면 좋겠다. 성능보다는 어떤 식으로 의사결정을 하고, 어떤 근거를 보고 개발하고 나아가는지를 볼 것 같다. Tuning에 시간을 많이 쓰지는 마라.
* Slide에 실험에 대해서 요약을 했으면 좋겠다. 시행착오에 대해서 얘기를 해줬으면 좋겠다. Baseline부터 시작해서 왜 이걸 선택을 하게 됐는지가 드러났으면 좋겠다.
* 최종 발표에 대해서 데이터셋을 더 공개를 하려고 한다. 연립방정식 -> 연랍방정식 형태로 noisy한 형태임. 실제 production 환경과 비슷한 test validation을 할 것 같다. 지금 regex 방식이 아니라 noise에 대응할 수 있는 방식을 보고 싶다. 논리적인 과정을 보고 싶다. 
* Multi label이 아니라 multi class이다. ㅠㅠ
* 제 예상으로는 Open source를 잘 활용하고 있는 것 같다. 지금까지 진행한 건에 팀이 가장 크게 contribution한 게 어떤 거라고 생각하나?
  * 마지막 최종 발표 때 preprocessing에 대한 분석이 있으면 좋겠다. 어떤 preprocessing을 했을 때 "어느 정도의 성능향상을 가져왔다"는 주장이 더 좋을 것 같다.
  * 울 회사에서도 내부 BERT 모델을 제작해서 1000개 ~ 2000개를 분류하는 classification을 하고 있음.
* 분류할 수 없는 문제가 아니라, 실제로 class를 나눠 떨어지지는 않는다. 충분히 문제를 풀어보고 one label로 했었는데, multi labeling은 내부에서 논의가 나오고 있음. 
* 높은 평가를 드리는 게 아니라 점수 기반이 아니라, 문제 상황을 listup하고 해결하는 과정을 보고 싶다.성능을 높이기 위해서 어떤 문제가 있었는지, 어떤 게 요인이 제일 크고, 이걸 어떻게 해결한 건지를 보고 싶다.
* 어떤 쪽을 유도하기 위해서가 아니라 본인이 중요하다고 생각한 부분을 집중적으로 파면 되고, 그게 왜 중요한 지만 설득할 수 있으면 됨. Presentation을 할 때 그 부분이 부각될 수 있도록 하면 좋겠다. 결과를 보여준 것들은 Model 별로 보여준 느낌이 있어서... 어떤 방식으로 의사결정을 하고, 어떤 방향으로 나갔는지, 어떤 로직으로 방향을 결정했는지를 보여주면 좋겠다. 



### Question

* Noise가 있는 데이터셋을 train까지 시켜야 하는 건지, 아니면 test benchmark로 확인하는건지?
  * Training set은 제공되지 않을 듯. API 하나만 추가될 예정.
  * **새로 제공되는 Evaluation 데이터셋을 바탕으로 전처리를 하라**
  * Preprocessing Module for test 1 vs Preprocessing Module for test 2가 갈릴텐데?
    * 전체를 아우를 수 있는 게 가능한가?
    * if statement로 noisy한 부분을 할 수 있게 하는 게 좋을 듯: Autoencoder같은 걸로 noise detection을 해서 지우는 작업을 할 수 있을 듯. 
    * 틀린 글자 삽입도 Data Augmentation 과정 중 하나니까 고려를 해봅시다.
* Pretrained된 BERT 토크나이저를 우리에게 주시는 건가요?
  * 현재 논의 중. Token에 따른 임베딩 벡터를 주는 걸로 생각하는 중. 문서로 제공하겠습니다.
* 어떤 데이터셋으로 pretrained된 건지? Base가 KoBERT인 건가?
  * 대답 드리기는 힘들다. Quanda 내부 데이터로 pretrain시킨 것임. ELECTRA 모델 기반으로 했고, 학습 기간은 10~20일 정도. 
* Noise가 있는 evaluation 데이터셋 문제 개수로 따진 사이즈는?
  * 지금 test set이랑 비슷하게 가져가려고 함. 분포도 맞춰가려고 함.
  * 독립적으로 sampling 된 문제입니다. 다만 중단원의 분포는 맞출 예정.
* 각 중단원 수가 현 고등 교육과정의 중단원 수와 다른데 혹시 중단원의 기준이 어떻게 되는지?
  * 완벽하게는 잘 모름. 이쪽 부분을 담당하시던 분이 지정해주시고 만드는 걸로 알고 있음.
  * 즉 교육과정의 중단원과는 관련이 있지만, 정확하게 답변하기는 어려움. 
* Classification으로 풀 수 없다고 생각되는 문제들: "다음 중 옳지 않은 것은?" 등
  * 의도한 건 아니었음.
  * 도저히 풀 수 없는 문제들은 다른 분들도 똑같으니 신경 X
* 처음에 train 데이터를 만들 때, OCR로 만드신 건가요?
  * 이건 전부 다 typing한 거다. test2는 OCR과 환경이 비슷할 거임.
  * Typing은 직원분들이 다 하신 거?: 생성하는 팀이 따로 있음. 내부에서 제작을 한 걸로 알고 있음.
* 기준 관련해서 질문을 더 드리자면, 경량화를 시킨다면 (적은 자원 or 적은 training 시간) 그것도 기준으로 볼까요?
  * 고건 당장 보고 있지는 않음. 현업에서는 중요한 것이긴 하다. engineering 적인 것은 보지 않겠다. 



## Model 개선 엑셀 시트

각 실험 별 어떤 걸 바꿨는지 정리를 해야 할 것 같다.  **Slack에 표를 만들어서 공유할 수 있음. Slack을 사용해보자.** 

* Model Name
* Preprocessing: white space 추가 등. Github commit url을 첨부
* HyperParameter Tuning
* Validation F1 score
* Model에서 변화: Early Stopping 등
* 몇 Epoch 돌렸는지
* Colab Notebook URL

## Preprocessing To-do

* Pipe ( | ) 처리하면 더 할 게 없을 것 같음.
  * CSV에서 replace all해서 "pipe"로 바꾼 다음에 regex 처리 하겠음.
* bar은 기호 하나로 바꾸면 더 좋을 것 같음.
* () 중복처리
* ^ summation에 대한 처리는 아직

## Preprocessing 방향 세분화 

각 시행 별로 성능 향상 보여주기. **각 version 별로 나눠서 모델 성능을 평가하는 게 좋겠다.** 깃헙 commit log에 적어놓기.

* Roman Letters in english (alpha, beta...) -> ɑ, β
* 영문 -> 한글
* 영문 -> 특수문자
* White Space
* Data Augmentation
  * Python Regex으로 noise 발생
  * TFIDF or Word2vec으로 noise 발생
  * BERT으로 noise 발생













